<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>More AWS things I learned the hard way: S3 best practices and VPCs | </title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="To make a long, mostly whiny story short, as part of my current role,
I&rsquo;ve been doing a lot of fighting with AWS to help support my team.
Some of the things I&rsquo;ve learned along the way are probably not obvious if you, like me, are relying
mostly on AWS docs and other people&rsquo;s advice, so I thought I&rsquo;d collect some of them here.

Best practices for storing big data on S3">
    <meta name="generator" content="Hugo 0.148.1">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.8d048772ae72ab11245a0e296d1f2a36d3e3dd376c6c867394d6cc659c68fc37.css" >




    


    
      

    

    

    
      <link rel="canonical" href="https://szeitlin.github.io/posts/engineering/aws-s3-vpc/">
    

    <meta property="og:url" content="https://szeitlin.github.io/posts/engineering/aws-s3-vpc/">
  <meta property="og:title" content="More AWS things I learned the hard way: S3 best practices and VPCs">
  <meta property="og:description" content="To make a long, mostly whiny story short, as part of my current role, I’ve been doing a lot of fighting with AWS to help support my team.
Some of the things I’ve learned along the way are probably not obvious if you, like me, are relying mostly on AWS docs and other people’s advice, so I thought I’d collect some of them here.
Best practices for storing big data on S3">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2018-12-02T16:29:47-08:00">
    <meta property="article:modified_time" content="2018-12-02T16:29:47-08:00">

  <meta itemprop="name" content="More AWS things I learned the hard way: S3 best practices and VPCs">
  <meta itemprop="description" content="To make a long, mostly whiny story short, as part of my current role, I’ve been doing a lot of fighting with AWS to help support my team.
Some of the things I’ve learned along the way are probably not obvious if you, like me, are relying mostly on AWS docs and other people’s advice, so I thought I’d collect some of them here.
Best practices for storing big data on S3">
  <meta itemprop="datePublished" content="2018-12-02T16:29:47-08:00">
  <meta itemprop="dateModified" content="2018-12-02T16:29:47-08:00">
  <meta itemprop="wordCount" content="1081">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="More AWS things I learned the hard way: S3 best practices and VPCs">
  <meta name="twitter:description" content="To make a long, mostly whiny story short, as part of my current role, I’ve been doing a lot of fighting with AWS to help support my team.
Some of the things I’ve learned along the way are probably not obvious if you, like me, are relying mostly on AWS docs and other people’s advice, so I thought I’d collect some of them here.
Best practices for storing big data on S3">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">More AWS things I learned the hard way: S3 best practices and VPCs</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2018-12-02T16:29:47-08:00">December 2, 2018</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>To make a long, mostly whiny story short, as part of my current role,
I&rsquo;ve been doing a lot of fighting with AWS to help support my team.</p>
<p>Some of the things I&rsquo;ve learned along the way are probably not obvious if you, like me, are relying
mostly on AWS docs and other people&rsquo;s advice, so I thought I&rsquo;d collect some of them here.</p>
<hr>
<p><strong>Best practices for storing big data on S3</strong></p>
<p>___<em>Think about your bucket structure</em></p>
<p>One of the things I ran into in my previous role was how hard it can be
to find a specific file in s3 if your file naming isn&rsquo;t set up to make this easy.</p>
<p>Although S3 is not a real filesystem, you can almost treat it like one, but it can take a little
bit of finagling to write your code to help with this issue.</p>
<p>So I recommend against, for example, putting dates in your filenames, like so:</p>
<p>Don&rsquo;t do this: <em>s3://mybucket/2018-12-02_server-1_file-1.csv</em></p>
<p>Why not?</p>
<p>Here&rsquo;s the scenario: let&rsquo;s say you&rsquo;re looking for a specific file from server 13 on Nov 12th.
But you can&rsquo;t grep for it because the file naming conventions changed at some point
and nobody&rsquo;s sure what the structure was at this particular point in time.</p>
<p>In order to make sure it exists, find out how big it is, and copy it to your machine,
you now have to list your <em>entire</em> bucket.</p>
<p>This is going to take a while. Like, probably minutes if your
data has been accumulating for a few years, especially if you have a lot of files.</p>
<p>In fact, the list
might be so long that you won&rsquo;t be able to scroll through it all, you might have to write it out
to a file and then search through that.</p>
<p>I say this from experience, having worked with legacy log files that were stored this way.</p>
<hr>
<p>___<em>Coalesce</em></p>
<p>I recommend against using whatever the default output is, if you&rsquo;re writing out from a cluster,
for example, with something like spark.</p>
<p>Most cluster systems will by default write out 1 file per partition or node,
which means you end up with a gazillion tiny files of different sizes and it rarely makes sense for
any downstream usage.</p>
<p>Instead, I recommend grouping, coalescing, and compressing your files.</p>
<p>___<em>Add slashes into your filenames.</em></p>
<p>This (artifically) groups your files by year, month, and day.</p>
<p>Then you can coalesce them down into, say, 1 file per day or per hour
(depending on how much data you have).</p>
<p><strong>I recommend a structure more like this:</strong> <em>s3://mybucket/2018/12/02/platform_name/file-1.gz</em></p>
<p>Now, when you list your bucket, you can easily jump to the date and platform you want, and
download a minimum number of files that meet your criteria.</p>
<hr>
<p>___<em>Think about your bucket policies and locations</em></p>
<p>Although new S3 buckets are now global, up until very recently they were restricted to certain regions for a variety
of reasons - availability, price, and latency are the main ones.</p>
<p>So if you&rsquo;re working with legacy buckets, remember that some AWS services will still restrict access if you don&rsquo;t
pass the correct region to go with the bucket (ask me how I know this after spending a couple of days
tracking down an AccessDenied error from EC2&hellip;).</p>
<p>Until recently, setting up strict bucket policies was sort of optional, but thanks to another recent change
in security measures, now you&rsquo;ll have to set explicit policies or you&rsquo;re going to find yourself
locked out of your own buckets.</p>
<p>And don&rsquo;t forget that if you use any kind of encryption, if you&rsquo;re trying to access those buckets from
another account, you have to use custom KMS keys,
and create an IAM role that has access to your encryption keys.</p>
<hr>
<p><strong>Some things I learned about VPCs</strong></p>
<p>As part of the Pachyderm setup (see separate post about that),
I had to set up a connection to our Redshift instance, which
lives on a separate account from our kubernetes cluster.</p>
<p>I tried following the instructions in the AWS docs, because it seemed pretty straightforward, but
as with my experience trying to set up cross-account access for EMR (see separate post about that),
it just didn&rsquo;t work.</p>
<p>Again, I couldn&rsquo;t
figure out why because I wasn&rsquo;t getting any useful feedback like error messages or logs that could
point me toward what I was missing.</p>
<p>After a lot of digging around in docs and asking for help from a variety of people online,
I finally found someone who was willing to help me troubleshoot (thanks, Drew Davies!).</p>
<p>What I ended up having to do was set up a new, custom VPC for Redshift.</p>
<p><em><strong>Hint 1: AWS doesn&rsquo;t really want you to use the default VPC for anything.</strong></em></p>
<p>It seems like the default VPC is there mostly as a way to make it easy to get started prototyping,
but for production it&rsquo;s of limited utility because there are a lot of features that
just don&rsquo;t work with the default VPC. (In this regard, it&rsquo;s similar
to how they won&rsquo;t let you use default encryption on s3 with cross-account access.)</p>
<p>So to solve my problem, I had to create a new VPC for Redshift,
update the IAM roles, and redeploy Redshift inside the new VPC. All of that worked as described
in the AWS docs.</p>
<p>Then I had to create the peering connection,
create new security groups,
and add the CIDR blocks to the routing tables.</p>
<p>The AWS docs were ok for getting me this far. I had to do a bit of reading about CIDR blocks.</p>
<p>Then I logged into the kubernetes cluster and tried to access the database, but it didn&rsquo;t work.</p>
<p><em><strong>Hint 2: You have to turn on DNS resolution for the peering connection.</strong></em></p>
<p>Finally, with Drew&rsquo;s help, we figured out that I needed to
go into a hidden menu and click a box for DNS resolution on the peering connection.
I had to do this on <em>both accounts</em>.</p>
<p>Then I had to wait a while before it propagated.</p>
<p>This is another thing that&rsquo;s frustrating about AWS - they
don&rsquo;t do a great job of telling you what you can expect might happen instantaneously vs. what might take
a few minutes.</p>
<p>This was all poorly documented and I genuinely don&rsquo;t understand why it&rsquo;s not the default
when you&rsquo;re setting up a peering connection between two VPCs.</p>
<p>After all that, I heard another friend struggling with the same problem, which is why I wanted to write
this up. Sorry it took so long for me to publish it!</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="https://szeitlin.github.io/" >
    &copy; 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
