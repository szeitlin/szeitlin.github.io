<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Testing on </title>
    <link>https://szeitlin.github.io/tags/testing/</link>
    <description>Recent content in Testing on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 Nov 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://szeitlin.github.io/tags/testing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Airflow</title>
      <link>https://szeitlin.github.io/posts/airflow/airflow-for-hands-off-etl/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://szeitlin.github.io/posts/airflow/airflow-for-hands-off-etl/</guid>
      <description>&lt;h1 id=&#34;airflow-for-hands-off-etl&#34;&gt;Airflow for hands-off ETL&lt;/h1&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Almost exactly a year ago, I joined &lt;a href=&#34;https://www.yahoo.com&#34;&gt;Yahoo&lt;/a&gt;, which more recently became &lt;a href=&#34;https://www.oath.com&#34;&gt;Oath&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The team I joined is called the Product Hackers, and we work with large amounts of data. By large amounts I meant, billions of rows of log data.&lt;/p&gt;&#xA;&lt;p&gt;Our team does both ad-hoc analyses and ongoing machine learning projects. In order to support those efforts, our team had initially written scripts to parse logs and run them with cron to load the data into Redshift on AWS. After a while, it made sense to move to &lt;a href=&#34;http://pythonhosted.org/airflow/&#34;&gt;Airflow&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A tutorial within a tutorial on building reusable models with scikit-learn</title>
      <link>https://szeitlin.github.io/posts/engineering/within-every-tutorial-is-another-tutorial/</link>
      <pubDate>Mon, 12 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://szeitlin.github.io/posts/engineering/within-every-tutorial-is-another-tutorial/</guid>
      <description>&lt;p&gt;Things I learned while following &lt;a href=&#34;http://blog.districtdatalabs.com/building-a-classifier-from-census-data/&#34;&gt;a tutorial&lt;/a&gt; on how to build reusable models with scikit-learn.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;When in doubt, go back to pandas.&lt;/li&gt;&#xA;&lt;li&gt;When in doubt, write tests.&lt;/li&gt;&#xA;&lt;li&gt;When in doubt, write helper methods to wrap existing objects, rather than creating new objects.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;ingesting-clean-data-is-easy-right&#34;&gt;Ingesting &amp;ldquo;clean&amp;rdquo; data is easy, right?&lt;/h2&gt;&#xA;&lt;p&gt;Step 1 of this tutorial began with downloading data using &lt;a href=&#34;http://requests.readthedocs.io/en/master/&#34;&gt;requests&lt;/a&gt;, and saving that to a csv file. So I did that. I&amp;rsquo;ve used requests before, I had no reason to think it wouldn&amp;rsquo;t work. It looked like it worked.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test-driven data pipelining</title>
      <link>https://szeitlin.github.io/posts/engineering/test-driven-data-pipelining/</link>
      <pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://szeitlin.github.io/posts/engineering/test-driven-data-pipelining/</guid>
      <description>&lt;h2 id=&#34;when-to-test-and-why&#34;&gt;When to test, and why:&lt;/h2&gt;&#xA;&lt;p&gt;• Write a test for every method.&lt;/p&gt;&#xA;&lt;p&gt;• Write a test any time you find a bug! Then make sure the test passes after you fix the bug.&lt;/p&gt;&#xA;&lt;p&gt;• Think of tests as showing how your code should be used, and write them accordingly. The next person who&amp;rsquo;s going to&#xA;edit your code, or even just use your code, should be able to refer to your tests to see what&amp;rsquo;s happening.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
